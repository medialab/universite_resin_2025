{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c09c1436-a82c-4c2a-814d-63840b959898",
   "metadata": {},
   "source": [
    "# Vllm\n",
    "\n",
    "<center>\n",
    "<img src=\"https://docs.vllm.ai/en/stable/assets/logos/vllm-logo-text-light.png\" alt=\"vllm logo\" width=\"400\"/>\n",
    "</center>\n",
    "\n",
    "Vllm is less cool and has an ugly logo :) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa73c86-e700-4284-91f1-fd11ebdc1d8e",
   "metadata": {},
   "source": [
    "### ðŸ³ï¸ Open the cc-in2p3 Jupyter Notebook launcher at \n",
    "\n",
    "> `https://notebook.cc.in2p3.fr`\n",
    "\n",
    "\n",
    "launch a new terminal session\n",
    "\n",
    "<center>\n",
    "    <img src=\"https://icon-library.com/images/bash-icon/bash-icon-6.jpg\" alt=\"bash is also cool\" width=\"200\"/>\n",
    "</center>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b631187b-0a99-48b5-b55e-8aed1df727f0",
   "metadata": {},
   "source": [
    "and execute therein the following commands to load the previously prepared enviroment with vllm installed\n",
    "\n",
    "```bash\n",
    "# Set up Modules\n",
    "source /etc/profile.d/modules.sh\n",
    "module purge\n",
    "module load Programming_Languages/python/3.11.4\n",
    "\n",
    "# Activate conda environment\n",
    "source /pbs/throng/training/universite-hiver/wschooloptim/vllm/bin/activate\n",
    "\n",
    "# Enforce correct site-packages\n",
    "unset PYTHONPATH\n",
    "export PYTHONPATH=/pbs/throng/training/universite-hiver/wschooloptim/vllm/lib/python3.11/site-packages/\n",
    "\n",
    "# Ollama and HF cache setting\n",
    "export HUGGINGFACE_HUB_CACHE=/pbs/throng/training/universite-hiver/cache/huggingface\n",
    "export OLLAMA_MODELS=/pbs/throng/training/universite-hiver/cache/ollama/models\n",
    "```\n",
    "\n",
    "launch an vllm server\n",
    "\n",
    "```bash\n",
    "vllm serve HuggingFaceH4/zephyr-7b-beta --max_model_len=10000 --guided_decoding_backend=xgrammar --seed=1  --tensor-parallel-size 1 &\n",
    "```\n",
    "\n",
    "Create one request to ollama's server:\n",
    "\n",
    "```bash\n",
    "curl -X POST http://localhost:8000/v1/chat/completions \\\n",
    "-H \"Content-Type: application/json\" \\\n",
    "-d '{\n",
    "  \"model\": \"HuggingFaceH4/zephyr-7b-beta\",\n",
    "  \"messages\": [{ \"role\": \"user\", \"content\": \"Who won the last presidential elections in France?\" }]\n",
    "}'\n",
    "```\n",
    "\n",
    "Keep that terminal open for the rest of the tutorial, we will check therein ollama server logs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4317ec75-d0a9-4316-b7b2-b304a525c78d",
   "metadata": {},
   "source": [
    "### ðŸ´ Open another cc-in2p3 Jupyter Notebook launcher at \n",
    "\n",
    "> `https://notebook.cc.in2p3.fr`\n",
    "\n",
    "\n",
    "and launch now the Notebook called `Python 3.13 (Ollama)`.\n",
    "\n",
    "We will work from now in that notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf9ff749-027b-46ac-b277-073328f88af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Displaying \u001b[36m3\u001b[0m/\u001b[36m21\u001b[0m cols from \u001b[36m10\u001b[0m rows of \u001b[2m<stdin>\u001b[0m\n",
      "\u001b[2mâ”Œâ”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\u001b[0m\n",
      "\u001b[2mâ”‚ \u001b[0m\u001b[2m-\u001b[0m\u001b[2m â”‚ \u001b[0m\u001b[1mcreated_at         \u001b[0m\u001b[2m â”‚ \u001b[0m\u001b[1mtweet_id      \u001b[0m\u001b[2m â”‚ â€¦\u001b[0m\u001b[2m â”‚ \u001b[0m\u001b[1mcollected_at                 \u001b[0m\u001b[2m â”‚\u001b[0m\n",
      "\u001b[2mâ”œâ”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\u001b[0m\n",
      "\u001b[2mâ”‚ \u001b[0m\u001b[2m0\u001b[0m\u001b[2m â”‚ \u001b[0m\u001b[35m2020-10-16 01:19:38\u001b[0m\u001b[2m â”‚ \u001b[0m\u001b[31m1.31691164952â€¦\u001b[0m\u001b[2m â”‚ â€¦\u001b[0m\u001b[2m â”‚ \u001b[0m\u001b[35m2020-10-21 02:33:39.914774259\u001b[0m\u001b[2m â”‚\u001b[0m\n",
      "\u001b[2mâ”‚ \u001b[0m\u001b[2m1\u001b[0m\u001b[2m â”‚ \u001b[0m\u001b[35m2020-10-15 18:06:56\u001b[0m\u001b[2m â”‚ \u001b[0m\u001b[31m1.31680275680â€¦\u001b[0m\u001b[2m â”‚ â€¦\u001b[0m\u001b[2m â”‚ \u001b[0m\u001b[35m2020-10-21 01:41:43.630185015\u001b[0m\u001b[2m â”‚\u001b[0m\n",
      "\u001b[2mâ”‚ \u001b[0m\u001b[2m2\u001b[0m\u001b[2m â”‚ \u001b[0m\u001b[35m2020-10-15 00:07:37\u001b[0m\u001b[2m â”‚ \u001b[0m\u001b[31m1.31653113467â€¦\u001b[0m\u001b[2m â”‚ â€¦\u001b[0m\u001b[2m â”‚ \u001b[0m\u001b[35m2020-10-21 00:00:54.489613435\u001b[0m\u001b[2m â”‚\u001b[0m\n",
      "\u001b[2mâ”‚ \u001b[0m\u001b[2m3\u001b[0m\u001b[2m â”‚ \u001b[0m\u001b[35m2020-10-15 08:38:01\u001b[0m\u001b[2m â”‚ \u001b[0m\u001b[31m1.31665958287â€¦\u001b[0m\u001b[2m â”‚ â€¦\u001b[0m\u001b[2m â”‚ \u001b[0m\u001b[35m2020-10-21 00:43:07.582933275\u001b[0m\u001b[2m â”‚\u001b[0m\n",
      "\u001b[2mâ”‚ \u001b[0m\u001b[2m4\u001b[0m\u001b[2m â”‚ \u001b[0m\u001b[35m2020-10-16 01:30:04\u001b[0m\u001b[2m â”‚ \u001b[0m\u001b[31m1.31691427389â€¦\u001b[0m\u001b[2m â”‚ â€¦\u001b[0m\u001b[2m â”‚ \u001b[0m\u001b[35m2020-10-21 02:37:50.543179243\u001b[0m\u001b[2m â”‚\u001b[0m\n",
      "\u001b[2mâ”‚ \u001b[0m\u001b[2m5\u001b[0m\u001b[2m â”‚ \u001b[0m\u001b[35m2020-10-15 08:03:42\u001b[0m\u001b[2m â”‚ \u001b[0m\u001b[31m1.31665094463â€¦\u001b[0m\u001b[2m â”‚ â€¦\u001b[0m\u001b[2m â”‚ \u001b[0m\u001b[35m2020-10-21 00:37:07.356253320\u001b[0m\u001b[2m â”‚\u001b[0m\n",
      "\u001b[2mâ”‚ \u001b[0m\u001b[2m6\u001b[0m\u001b[2m â”‚ \u001b[0m\u001b[35m2020-10-16 10:29:56\u001b[0m\u001b[2m â”‚ \u001b[0m\u001b[31m1.31705013636â€¦\u001b[0m\u001b[2m â”‚ â€¦\u001b[0m\u001b[2m â”‚ \u001b[0m\u001b[35m2020-10-21 03:28:28.635848751\u001b[0m\u001b[2m â”‚\u001b[0m\n",
      "\u001b[2mâ”‚ \u001b[0m\u001b[2m7\u001b[0m\u001b[2m â”‚ \u001b[0m\u001b[35m2020-10-16 07:39:55\u001b[0m\u001b[2m â”‚ \u001b[0m\u001b[31m1.31700734863â€¦\u001b[0m\u001b[2m â”‚ â€¦\u001b[0m\u001b[2m â”‚ \u001b[0m\u001b[35m2020-10-21 03:21:35.409676897\u001b[0m\u001b[2m â”‚\u001b[0m\n",
      "\u001b[2mâ”‚ \u001b[0m\u001b[2m8\u001b[0m\u001b[2m â”‚ \u001b[0m\u001b[35m2020-10-15 16:39:48\u001b[0m\u001b[2m â”‚ \u001b[0m\u001b[31m1.31678082824â€¦\u001b[0m\u001b[2m â”‚ â€¦\u001b[0m\u001b[2m â”‚ \u001b[0m\u001b[35m2020-10-21 01:29:58.867252818\u001b[0m\u001b[2m â”‚\u001b[0m\n",
      "\u001b[2mâ”‚ \u001b[0m\u001b[2m9\u001b[0m\u001b[2m â”‚ \u001b[0m\u001b[35m2020-10-15 15:59:08\u001b[0m\u001b[2m â”‚ \u001b[0m\u001b[31m1.31677059298â€¦\u001b[0m\u001b[2m â”‚ â€¦\u001b[0m\u001b[2m â”‚ \u001b[0m\u001b[35m2020-10-21 01:24:58.009601380\u001b[0m\u001b[2m â”‚\u001b[0m\n",
      "\u001b[2mâ””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! xan tail hashtag_donaldtrump_6k_joebiden_25k_shuffled.csv | xan v "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847b6181-35c7-4cbd-872a-d33c9a6b6f48",
   "metadata": {},
   "source": [
    "##### Import python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27085d2a-86af-438e-939b-0887477cf9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "import asyncio\n",
    "from openai import OpenAI\n",
    "from string import Template\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9f2b07-a4e3-4e51-884a-67241aea0aa3",
   "metadata": {},
   "source": [
    "##### Load inputs and create asynchronous iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9237dfb-bb1e-4ee5-84b6-823ddac08a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 31000 tweets tooks 0.20054149627685547 seconds.\n"
     ]
    }
   ],
   "source": [
    "input_file = \"hashtag_donaldtrump_6k_joebiden_25k_shuffled.csv\"\n",
    "content_column = 'tweet'\n",
    "\n",
    "start = time.time()\n",
    "with open(input_file, 'r') as f:\n",
    "    tweets = [d[content_column] for d in csv.DictReader(f)]\n",
    "end = time.time()\n",
    "print(f\"Loading {len(tweets)} tweets tooks {end - start} seconds.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d520103f-ab84-4377-a9b6-3abfc672e541",
   "metadata": {},
   "source": [
    "##### Load choices and extra body dictionary with structured_outputs options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c0f58bf-989d-4d6d-a3d7-a85ee450e9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'structured_outputs': {'choice': ['Biden', 'Trump', 'None', '']}}\n"
     ]
    }
   ],
   "source": [
    "choices_file = \"multiple_us.txt\"\n",
    "with open(choices_file, 'r') as f:\n",
    "    choice = f.read().split(\"\\n\")\n",
    "\n",
    "extra_body = {\"structured_outputs\": {\"choice\": choice}}\n",
    "\n",
    "print(extra_body)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3b6168-755b-4d74-84cc-e831b7598580",
   "metadata": {},
   "source": [
    "##### Load instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fc5f4fa-799b-4a0e-8e29-975daf315826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please classify the following social media message (posted in the weeks leading up to the 2025 Chilean presidential election) according to whether it explicitly expresses the intention  of the author to vote for or calls for a vote for any of the candidates in that election: Jeannette Jara, JosÃ© Antonio Kast, Johannes Kaiser, Evelyn Matthei, Franco Parisi, Eduardo ArtÃ©s, Harold Mayne-Nichols, Marco EnrÃ­quez-Ominami (also known as MEO), or whether it expresses neither of these intentions. Your answer should be based solely on the information contained in the message. Do not confuse a simple mention of a candidate with an intention or call to vote for him. Do not assume that a message containing only positive opinions about a particular candidate explicitly expresses the intention to vote for that candidate. Do not assume that a message corresponding to the opinions or political positions of a particular candidate necessarily expresses the intention to vote for that candidate. Do not confuse retweets, indirect speech or a quote to another person with the opinion of the author of the message. Be concise and respond only with the last name or the word 'None'. Here is the message: ${tweet}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "instructions_file = \"/pbs/throng/training/universite-hiver/wschooloptim/resinwinterschool/instructions.txt\"\n",
    "with open(instructions_file, 'r') as f:\n",
    "    instructions = f.read()\n",
    "print(instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2291f345-d9a1-4651-b0dc-72836f48a4cb",
   "metadata": {},
   "source": [
    "##### Create vllm openai client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d628a66e-7d35-48c3-b832-1e09e5ed8680",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:8000/v1\", # this is the same HOST given at ollama server launching\n",
    "    api_key=\"EMPTY\"  # required but unused\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71354908-8b3a-4930-8b82-718d5a2a262f",
   "metadata": {},
   "source": [
    "##### Create async functions to asynchronously request llm server\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a43adab-3154-48b2-89dd-0feffd5715e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def inputsIterator(start, end):\n",
    "    for tweet in tweets[start:end]:\n",
    "        yield tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2db16264-610d-4459-b6a4-3c81a1db1cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def doCompletetion(tweet):\n",
    "    return client.chat.completions.create(\n",
    "        model=client.models.list().data[0].id,\n",
    "        messages=[{\n",
    "            'role': 'user',\n",
    "            'content': Template(instructions).substitute(tweet=tweet)\n",
    "        }],\n",
    "        extra_body=extra_body,\n",
    "        max_completion_tokens=16,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e874e646-bc18-41cb-a2cb-8146cbd0f5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_all(start, end):\n",
    "    # Asynchronously call the function for each prompt\n",
    "    tasks = [\n",
    "        doCompletetion(tweet)\n",
    "        async for tweet in inputsIterator(start, end)\n",
    "    ]\n",
    "    # Gather and run the tasks concurrently\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4250237-a5aa-4383-8c89-58d5fd048b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Annotationg 100 tweets took 6.010417222976685 seconds,\n",
      "this is 0.06010417222976685 seconds per tweet or\n",
      "0.6956501415482274 days for the whole database of 1000000 inputs.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run all courutines\n",
    "init_idx = 3449\n",
    "nb_tweets = 100\n",
    "\n",
    "start = time.time()\n",
    "results = asyncio.run(run_all(start=init_idx, end=init_idx + nb_tweets))\n",
    "elapsed = time.time() - start\n",
    "\n",
    "whole_db_nb_tweets = int(1e6)\n",
    "\n",
    "\n",
    "print(f\"\"\"\n",
    "Annotationg {nb_tweets} tweets took {elapsed} seconds,\n",
    "this is {elapsed / nb_tweets} seconds per tweet or\n",
    "{(1 / (24 * 3600)) * whole_db_nb_tweets * elapsed / nb_tweets} days for the whole database of {whole_db_nb_tweets} inputs.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed74b44e-079f-4246-8d8c-779f9b3e0a2a",
   "metadata": {},
   "source": [
    "> Annotationg 500 tweets took 27.607022047042847 seconds,\n",
    ">\n",
    "> tthis is 0.055214044094085694 seconds per tweet or\n",
    ">\n",
    "> 0.6670885399377953 days for the whole database of 1043873 inputs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 (vllm)",
   "language": "python",
   "name": "vllm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "57727863-230d-4fc6-91dd-15db09ea8a57",
   "metadata": {},
   "source": [
    "# üöÄü™ê‚ö° LLMs at Scale Tutorial\n",
    "\n",
    "\n",
    "### üî∏ Initialization\n",
    "\n",
    "- In this tutorial we will compare high-performance library for Large language models (LLMs), optimized for inference and serving, with more simple and lightweight tools.\n",
    "\n",
    "- The goal is to demostrate how inference oriented engines are able to optimizes CPU and GPU resources during inference, achieving significantly higher throughput.\n",
    "\n",
    "\n",
    "- To do this, we use Hugging Face's `transformers` library and `vllm` `vllm` and `ollama` frameworks.\n",
    "\n",
    "\n",
    "> [Transformers](https://huggingface.co/docs/transformers/index) Hugging Face Transformers is an open source library that provides easy access to thousands of machine learning models. Its versatility and large model hub make it a go-to tool for both beginners and researchers to build AI applications with minimal effort.\n",
    "\n",
    "<center>\n",
    "<img src=\"https://replit.com/_next/image?url=https%3A%2F%2Fstorage.googleapis.com%2Freplit%2Fimages%2F1658792716197_0b6b847e5ff6197ab4d4186246f001ae.png&w=1920&q=75\n",
    "\" alt=\"ollama logo\" width=\"400\"/>\n",
    "</center>\n",
    "\n",
    "\n",
    "> [Ollama](https://docs.ollama.com/) is a simple and lightweight tool that allows LLMs to be run locally without complex configuration. \n",
    "\n",
    "<center>\n",
    "<img src=\"https://www.linuxtricks.fr/upload/ollama-logo.png\" alt=\"ollama logo\" width=\"300\"/>\n",
    "</center>\n",
    "\n",
    "> [Vllm](vllm.ai) is an open-source library designed to enhance the performance of LLMs by improving their efficiency in processing and memory management. \n",
    "\n",
    "<center>\n",
    "<img src=\"https://docs.vllm.ai/en/stable/assets/logos/vllm-logo-text-light.png\" alt=\"vllm logo\" width=\"300\"/>\n",
    "</center>\n",
    "\n",
    "- We will use both for loading, managing, and querying an LLMs. In order to make the make a siple faire comparaison, we will instantiate a ollama and a vllm server and request them through `OpenAI`'s integreted API.\n",
    "\n",
    "<center>\n",
    "<img src=\"https://logos-world.net/wp-content/uploads/2024/08/OpenAI-Logo-500x281.png\" alt=\"openai logo\" width=\"300\"/>\n",
    "</center>\n",
    "\n",
    "- The model used is `Zephyr-7B-Œ≤`. [Zephyr-7B-Œ≤](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta) is a small language models trained to act as helpful assistants. It is a fine-tuned version of [mistralai/Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1) that was trained on on a mix of publicly available and synthetic dataset.\n",
    "\n",
    "<center>\n",
    "<img src=\"https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha/resolve/main/thumbnail.png\" alt=\"zephyr image\" width=\"300\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e205df-f166-4391-8ed0-d099f445499a",
   "metadata": {},
   "source": [
    "## üè≥Ô∏è First steps\n",
    "\n",
    "Open cc-in2p3 Jupyter Notebook launcher at \n",
    "\n",
    "> `https://notebook.cc.in2p3.fr`\n",
    "\n",
    "### 1Ô∏è‚É£ Clone the Github repository\n",
    "\n",
    "All the materials and files needed for this tutorial are available at\n",
    "\n",
    "> `https://github.com/jimenaRL/resinwinterschool/`\n",
    "\n",
    "Clone the gitlab repository directly from Jupyter Notebook platform and go to the folder.\n",
    "\n",
    "\n",
    "```bash\n",
    "    $ git clone https://github.com/jimenaRL/resinwinterschool.git\n",
    "    $ cd resinwinterschool\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3c07c1-8fcf-4f9c-8d86-fca8853d02aa",
   "metadata": {},
   "source": [
    "### 2Ô∏è‚É£ GPU card(s) monitoring\n",
    "\n",
    "<center>\n",
    "<img src=\"https://docs.nvidia.com/bionemo-framework/1.10/_static/nvidia-logo-horiz-rgb-blk-for-screen.png\" alt=\"nvidia logo\" width=\"300\"/>\n",
    "</center>\n",
    "\n",
    "\n",
    "In order to check what is going on with the GPU cards(s), whe will use NVIDIA's System Management Interface to management and monitoring GPU devices. \n",
    "\n",
    "\n",
    "```bash\n",
    "    $ nvidia-smi\n",
    "```\n",
    "\n",
    "You may run the utility directly from the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "542adf72-be1d-472f-90f6-5c300edb5018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec 15 22:47:21 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 575.57.08              Driver Version: 575.57.08      CUDA Version: 12.9     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA L40S                    Off |   00000000:02:00.0 Off |                    0 |\n",
      "| N/A   36C    P8             34W /  350W |       3MiB /  46068MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA L40S                    Off |   00000000:64:00.0 Off |                    0 |\n",
      "| N/A   36C    P8             34W /  350W |       3MiB /  46068MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756b1ab4-3f5f-46d1-9c28-a40584ba74d7",
   "metadata": {},
   "source": [
    "Or launch a new  `bash terminal` from the Jupyter Notebook launcher platform and execute therin the looped version to check information in real time:\n",
    "\n",
    "```bash\n",
    "    $ nvidia-smi -lms\n",
    "```\n",
    "\n",
    "and launch a new terminal session\n",
    "\n",
    "<center>\n",
    "    <img src=\"https://icon-library.com/images/bash-icon/bash-icon-6.jpg\" alt=\"bash is also cool\" width=\"200\"/>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae44fbd-387e-4f3c-b2bd-90f4a4917ba6",
   "metadata": {},
   "source": [
    "### 3Ô∏è‚É£ Kernel Creations\n",
    "\n",
    "We will use two different python virtual environements.\n",
    "\n",
    "A Python virtual environment is an isolated context in which Python code runs, consisting of an interpreter, libraries, and installed packages.\n",
    "This will help us to manage dependencies for the two different experiments separately, while running on the same server.\n",
    "\n",
    "##### üîπ vllm kernel\n",
    "\n",
    "Create a new directory in your home \n",
    "\n",
    "```bash\n",
    "    $ mkdir -p ~/.local/share/jupyter/kernels/vllm\n",
    "```\n",
    "\n",
    "and copy therein the json config file for vllm's environement kernel\n",
    "\n",
    "\n",
    "```bash\n",
    "    $ cp /pbs/throng/training/universite-hiver/wschooloptim/vllm/kernel.json ~/.local/share/jupyter/kernels/vllm/kernel.json\n",
    "```\n",
    "\n",
    "##### üîπ ollama kernel\n",
    "\n",
    "Similarly, create a new directory in your home \n",
    "\n",
    "```bash\n",
    "    $ mkdir -p ~/.local/share/jupyter/kernels/RAG_LLM\n",
    "```\n",
    "\n",
    "and copy therein the json config file for ollama's environement kernel\n",
    "\n",
    "\n",
    "```bash\n",
    "    $ cp /pbs/throng/training/Huma-Num/envs/RAG_LLM/jupyter-helper.sh ~/.local/share/jupyter/kernels/RAG_LLM/kernel.json\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc61f72b-8b38-4512-9276-aab1d0372951",
   "metadata": {},
   "source": [
    "### üèÅ We are ready to go !\n",
    "\n",
    "Now you can reload Jupyter Notebook launch and at \n",
    "\n",
    "> `https://notebook.cc.in2p3.fr`\n",
    "\n",
    "and follow the second part of this tutorial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 (vllm)",
   "language": "python",
   "name": "vllm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

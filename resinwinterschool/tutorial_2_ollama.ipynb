{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c09c1436-a82c-4c2a-814d-63840b959898",
   "metadata": {},
   "source": [
    "# ğŸ¦™ Ollama\n",
    "\n",
    "<center>\n",
    "    <img src=\"https://ollama.com/public/cloud.png\" alt=\"ollama is cool\" width=\"200\"/>\n",
    "    <img src=\"https://mintcdn.com/ollama-9269c548/w-L7kuDqk3_8zi5c/images/welcome.png?w=840&fit=max&auto=format&n=w-L7kuDqk3_8zi5c&q=85&s=cc3770b69817332b5961d0faecdce473\" alt=\"ollama working\" width=\"400\"/>\n",
    "</center>\n",
    "\n",
    "\n",
    "\n",
    "As we can see in these picture (^^), [Ollama](https://docs.ollama.com/) is a cool, simple and lightweight tool that allows LLMs to be run locally without complex configuration. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa73c86-e700-4284-91f1-fd11ebdc1d8e",
   "metadata": {},
   "source": [
    "### ğŸ³ï¸ Open the cc-in2p3 Jupyter Notebook launcher at \n",
    "\n",
    "> `https://notebook.cc.in2p3.fr`\n",
    "\n",
    "\n",
    "launch a new terminal session\n",
    "\n",
    "<center>\n",
    "    <img src=\"https://icon-library.com/images/bash-icon/bash-icon-6.jpg\" alt=\"bash is also cool\" width=\"200\"/>\n",
    "</center>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b631187b-0a99-48b5-b55e-8aed1df727f0",
   "metadata": {},
   "source": [
    "and execute therein the following commands to load the previously prepared enviroment with ollama installed\n",
    "\n",
    "```bash\n",
    "module load ollama\n",
    "```\n",
    "\n",
    "launch an ollama server\n",
    "\n",
    "```bash\n",
    "export OLLAMA_MODELS=/pbs/throng/training/universite-hiver/cache/ollama/models\n",
    "export CUDA_VISIBLES_DEVICES=GPU-99ec877c-53b2-49ba-a1ea-0b6f0d0f0c08; \n",
    "export OLLAMA_HOST=http://127.0.0.1:11435;\n",
    "export OLLAMA_CONTEXT_LENGTH=4096;\n",
    "export OLLAMA_KV_CACHE_TYPE=fp16;\n",
    "export OLLAMA_FLASH_ATTENTION=1; \n",
    "ollama serve &\n",
    "```\n",
    "\n",
    "and pull `zephyr:7b` model\n",
    "\n",
    "```bash\n",
    "ollama pull zephyr:7b\n",
    "```\n",
    "\n",
    "Check that `zephyr:7b` model is present and show some informations\n",
    "\n",
    "```bash\n",
    "ollama list\n",
    "ollama show zephyr:7b\n",
    "```\n",
    "\n",
    "Create one request to ollama's server:\n",
    "\n",
    "```bash\n",
    "curl -X POST http://localhost:11435/v1/chat/completions \\\n",
    "-H \"Content-Type: application/json\" \\\n",
    "-d '{\n",
    "  \"model\": \"zephyr:7b\",\n",
    "  \"messages\": [{ \"role\": \"user\", \"content\": \"Who won the last presidential elections in France?\" }]\n",
    "}'\n",
    "```\n",
    "\n",
    "Keep that terminal open for the rest of the tutorial, we will check therein ollama server logs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4317ec75-d0a9-4316-b7b2-b304a525c78d",
   "metadata": {},
   "source": [
    "### ğŸ´ Open another cc-in2p3 Jupyter Notebook launcher at \n",
    "\n",
    "> `https://notebook.cc.in2p3.fr`\n",
    "\n",
    "\n",
    "and launch now the Notebook called `Python 3.13 (Ollama)`.\n",
    "\n",
    "We will work from now in that notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d137327c-527a-401b-8209-349c055e1b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['OLLAMA_MODELS'] = \"/pbs/throng/training/universite-hiver/cache/ollama/models\"\n",
    "os.environ['PATH'] = os.environ['PATH']+\":\"+\"/pbs/throng/training/universite-hiver/wschooloptim/vllm/bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "760e373d-bb55-452a-acb0-50435fa6a4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa434695-895d-4130-8bf4-135aa79868ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PATH variable is set to\n",
      "\t/pbs/throng/training/Huma-Num/envs/RAG_LLM/bin:/pbs/software/redhat-9-x86_64/ollama/0.12.3/bin:/pbs/software/redhat-9-x86_64/anaconda/3.13/condabin:/pbs/software/redhat-9-x86_64/anaconda/3.13/bin:/usr/share/Modules/bin:/usr/local/cuda/bin:/pbs/software/redhat-9-x86_64/latex/2025/bin/x86_64-linux:/pbs/software/redhat-9-x86_64/jnp/3.11/bin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/pbs/throng/training/universite-hiver/wschooloptim/vllm/bin\n",
      "\n",
      "PYTHONPATH variable is set to\n",
      "\t/pbs/throng/training/Huma-Num/envs/RAG_LLM/lib/python3.13/site-packages:\n",
      "\n",
      "OLLAMA_MODELS variable is set to\n",
      "\t/pbs/throng/training/universite-hiver/cache/ollama/models\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nPATH variable is set to\\n\\t{os.environ['PATH']}\")\n",
    "\n",
    "print(f\"\\nPYTHONPATH variable is set to\\n\\t{os.environ['PYTHONPATH']}\")\n",
    "\n",
    "print(f\"\\nOLLAMA_MODELS variable is set to\\n\\t{os.environ['OLLAMA_MODELS']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf9ff749-027b-46ac-b277-073328f88af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   created_at\n",
      "1   tweet_id\n",
      "2   tweet\n",
      "3   likes\n",
      "4   retweet_count\n",
      "5   source\n",
      "6   user_id\n",
      "7   user_name\n",
      "8   user_screen_name\n",
      "9   user_description\n",
      "10  user_join_date\n",
      "11  user_followers_count\n",
      "12  user_location\n",
      "13  lat\n",
      "14  long\n",
      "15  city\n",
      "16  country\n",
      "17  continent\n",
      "18  state\n",
      "19  state_code\n",
      "20  collected_at\n"
     ]
    }
   ],
   "source": [
    "! xan headers hashtag_donaldtrump_6k_joebiden_25k_shuffled.csv  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb373404-7ba4-4425-b943-4092500471a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Displaying \u001b[36m3\u001b[0m/\u001b[36m21\u001b[0m cols from \u001b[36m5\u001b[0m rows of \u001b[2m<stdin>\u001b[0m\n",
      "\u001b[2mâ”Œâ”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\u001b[0m\n",
      "\u001b[2mâ”‚ \u001b[0m\u001b[2m-\u001b[0m\u001b[2m â”‚ \u001b[0m\u001b[1mcreated_at         \u001b[0m\u001b[2m â”‚ \u001b[0m\u001b[1mtweet_id      \u001b[0m\u001b[2m â”‚ â€¦\u001b[0m\u001b[2m â”‚ \u001b[0m\u001b[1mcollected_at                 \u001b[0m\u001b[2m â”‚\u001b[0m\n",
      "\u001b[2mâ”œâ”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\u001b[0m\n",
      "\u001b[2mâ”‚ \u001b[0m\u001b[2m0\u001b[0m\u001b[2m â”‚ \u001b[0m\u001b[35m2020-10-15 17:11:14\u001b[0m\u001b[2m â”‚ \u001b[0m\u001b[31m1.31678873900â€¦\u001b[0m\u001b[2m â”‚ â€¦\u001b[0m\u001b[2m â”‚ \u001b[0m\u001b[35m2020-10-21 01:34:14.156103349\u001b[0m\u001b[2m â”‚\u001b[0m\n",
      "\u001b[2mâ”‚ \u001b[0m\u001b[2m1\u001b[0m\u001b[2m â”‚ \u001b[0m\u001b[35m2020-10-16 01:33:07\u001b[0m\u001b[2m â”‚ \u001b[0m\u001b[31m1.31691504205â€¦\u001b[0m\u001b[2m â”‚ â€¦\u001b[0m\u001b[2m â”‚ \u001b[0m\u001b[35m2020-10-21 02:39:01.485517018\u001b[0m\u001b[2m â”‚\u001b[0m\n",
      "\u001b[2mâ”‚ \u001b[0m\u001b[2m2\u001b[0m\u001b[2m â”‚ \u001b[0m\u001b[35m2020-10-15 03:30:21\u001b[0m\u001b[2m â”‚ \u001b[0m\u001b[31m1.31658215729â€¦\u001b[0m\u001b[2m â”‚ â€¦\u001b[0m\u001b[2m â”‚ \u001b[0m\u001b[35m2020-10-21 00:24:44.610820432\u001b[0m\u001b[2m â”‚\u001b[0m\n",
      "\u001b[2mâ”‚ \u001b[0m\u001b[2m3\u001b[0m\u001b[2m â”‚ \u001b[0m\u001b[35m2020-10-15 00:00:36\u001b[0m\u001b[2m â”‚ \u001b[0m\u001b[31m1.31652936920â€¦\u001b[0m\u001b[2m â”‚ â€¦\u001b[0m\u001b[2m â”‚ \u001b[0m\u001b[35m2020-10-21 00:00:04.142618264\u001b[0m\u001b[2m â”‚\u001b[0m\n",
      "\u001b[2mâ”‚ \u001b[0m\u001b[2m4\u001b[0m\u001b[2m â”‚ \u001b[0m\u001b[35m2020-10-15 20:58:13\u001b[0m\u001b[2m â”‚ \u001b[0m\u001b[31m1.31684585938â€¦\u001b[0m\u001b[2m â”‚ â€¦\u001b[0m\u001b[2m â”‚ \u001b[0m\u001b[35m2020-10-21 02:05:10.566912994\u001b[0m\u001b[2m â”‚\u001b[0m\n",
      "\u001b[2mâ””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! xan slice -l 5 hashtag_donaldtrump_6k_joebiden_25k_shuffled.csv | xan v  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fb7b39f-1eb6-4542-9157-f9d0b3551b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mRow nÂ°0\u001b[0m\n",
      "\u001b[2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
      "tweet \u001b[32m#HunterBidenEmails #HunterBiden #BidenCrimeFamily #Biden #BidenHarris2020 #TwitterCensorship #censorship #FacebookCensorship https://t.co/2sD4rovM1f\u001b[0m\n",
      "\n",
      "\u001b[1mRow nÂ°1\u001b[0m\n",
      "\u001b[2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
      "tweet \u001b[32m\"It's just decency... I'm less concerned about me but the people [around me].\"\n",
      "\n",
      "THANK YOU for saying it for the people who need to learn empathy!! \n",
      "#biden #townhall #BidenTownHall #TownHalls https://t.co/lLV8GvPEUZ\u001b[0m\n",
      "\n",
      "\u001b[1mRow nÂ°2\u001b[0m\n",
      "\u001b[2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
      "tweet \u001b[32m\"Mostly peaceful\" #campaign-finance violations?\n",
      "#CorruptDemocrats #HunterBidenEmails #JoeBiden #ukraine #VOTE #Election2020 https://t.co/wfR6jjV6Mv\u001b[0m\n",
      "\n",
      "\u001b[1mRow nÂ°3\u001b[0m\n",
      "\u001b[2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
      "tweet \u001b[32m#Biden https://t.co/qMs0PmUev5\u001b[0m\n",
      "\n",
      "\u001b[1mRow nÂ°4\u001b[0m\n",
      "\u001b[2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
      "tweet \u001b[32m#Biden -- #Kamala is bought &amp; bossed. Just a raccoon &amp; token. Biden's been complicit in poisoning water, bombing other nations, &amp; deporting at alarming rates while giving corporations handouts. https://t.co/2ZLcs38kAs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! xan select tweet hashtag_donaldtrump_6k_joebiden_25k_shuffled.csv | xan slice -l 5 | xan f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847b6181-35c7-4cbd-872a-d33c9a6b6f48",
   "metadata": {},
   "source": [
    "##### Import python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27085d2a-86af-438e-939b-0887477cf9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "import asyncio\n",
    "from openai import OpenAI\n",
    "from string import Template\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9f2b07-a4e3-4e51-884a-67241aea0aa3",
   "metadata": {},
   "source": [
    "##### Load inputs and create asynchronous iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9237dfb-bb1e-4ee5-84b6-823ddac08a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 31000 tweets tooks 0.16350007057189941 seconds.\n"
     ]
    }
   ],
   "source": [
    "input_file = \"hashtag_donaldtrump_6k_joebiden_25k_shuffled.csv\"\n",
    "content_column = 'tweet'\n",
    "\n",
    "start = time.time()\n",
    "with open(input_file, 'r') as f:\n",
    "    tweets = [d[content_column] for d in csv.DictReader(f)]\n",
    "end = time.time()\n",
    "print(f\"Loading {len(tweets)} tweets tooks {end - start} seconds.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d520103f-ab84-4377-a9b6-3abfc672e541",
   "metadata": {},
   "source": [
    "##### Load choices and extra body dictionary with structured_outputs options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c0f58bf-989d-4d6d-a3d7-a85ee450e9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert in US politics.\n"
     ]
    }
   ],
   "source": [
    "system_content = \"You are an expert in US politics.\"\n",
    "print(system_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f653042-98cf-49f2-9bc4-b8a69251a621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'structured_outputs': {'choice': ['Biden', 'Trump', 'None']}}\n"
     ]
    }
   ],
   "source": [
    "choices_file = \"multiple_us.txt\"\n",
    "with open(choices_file, 'r') as f:\n",
    "    choice = f.read().split(\"\\n\")[:-1]\n",
    "\n",
    "extra_body = {\"structured_outputs\": {\"choice\": choice}}\n",
    "\n",
    "print(extra_body)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3b6168-755b-4d74-84cc-e831b7598580",
   "metadata": {},
   "source": [
    "##### Load instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fc5f4fa-799b-4a0e-8e29-975daf315826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please classify the following social media message (posted in the weeks leading up to the 2020 US presidential election) according to whether it explicitly expresses the intention  of the author to vote for or calls for a vote for any of the candidates in that election: Donald Trump or Joy Biden, or whether it expresses neither of these intentions. Your answer should be based solely on the information contained in the message. Do not confuse a simple mention of a candidate with an intention or call to vote for him. Do not assume that a message containing only positive opinions about a particular candidate explicitly expresses the intention to vote for that candidate. Do not assume that a message corresponding to the opinions or political positions of a particular candidate necessarily expresses the intention to vote for that candidate. Do not confuse retweets, indirect speech or a quote to another person with the opinion of the author of the message. Be concise and respond only with the last name or the word \"None\". Here is the message: ${tweet}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "instructions_file = \"user_prompt_voteintention_multiple_all_english_us.txt\"\n",
    "with open(instructions_file, 'r') as f:\n",
    "    instructions = f.read()\n",
    "print(instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2291f345-d9a1-4651-b0dc-72836f48a4cb",
   "metadata": {},
   "source": [
    "##### Create ollama openai client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71354908-8b3a-4930-8b82-718d5a2a262f",
   "metadata": {},
   "source": [
    "##### Create async functions to asynchronously request llm server\n",
    "\n",
    "`asyncio` is a library to write concurrent code using the async/await syntax.\n",
    "\n",
    "`asyncio` is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc.\n",
    "\n",
    "\n",
    "\n",
    "<center>\n",
    "    <img src=\"https://cdn.prod.website-files.com/5d2dd7e1b4a76d8b803ac1aa/5fc4d4d7dea01e43ee075dbf_B6oeReCTEE90L7UN3fOOyhgqQO_hY4OZvFUTmXcPMjYIByNrdkYaXRe0qaAxgz9BlenAkd4_hgcw5OXKk8mTLjn1u-BeqEoEtSbW-SPwLzCZ2yeW-3gPJLOoMbc910y6IvaVziHG.png\" alt=\"bash is also cool\" width=\"700\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d628a66e-7d35-48c3-b832-1e09e5ed8680",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    base_url=\"http://127.0.0.1:11435/v1/\", # this is the same HOST given at ollama server launching\n",
    "    api_key=\"EMPTY\"  # required but unused\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13d1e1e3-2c64-4c7a-8865-f7b410563ccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Model(id='llama3.1:8b', created=1766054138, object='model', owned_by='library'),\n",
       " Model(id='zephyr:7b', created=1766051915, object='model', owned_by='library'),\n",
       " Model(id='gpt-oss:20b', created=1765919676, object='model', owned_by='library')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.models.list().data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a43adab-3154-48b2-89dd-0feffd5715e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def inputsIterator(start, end):\n",
    "    for tweet in tweets[start:end]:\n",
    "        yield tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2db16264-610d-4459-b6a4-3c81a1db1cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def doCompletetion(tweet):\n",
    "    return client.chat.completions.create(\n",
    "        model='llama3.1:8b',\n",
    "        messages=[\n",
    "            {\n",
    "            'role': 'system',\n",
    "            'content': system_content,\n",
    "            },\n",
    "            {\n",
    "            'role': 'user',\n",
    "            'content': Template(instructions).substitute(tweet=tweet)\n",
    "        }],\n",
    "        extra_body=extra_body,\n",
    "        max_completion_tokens=16,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e874e646-bc18-41cb-a2cb-8146cbd0f5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_all(start, end):\n",
    "    # Asynchronously call the function for each prompt\n",
    "    tasks = [\n",
    "        doCompletetion(tweet)\n",
    "        async for tweet in inputsIterator(start, end)\n",
    "    ]\n",
    "    # Gather and run the tasks concurrently\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f4250237-a5aa-4383-8c89-58d5fd048b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Annotationg 10 tweets took 1.274366855621338 seconds,\n",
      "this is 0.12743668556213378 seconds per tweet or\n",
      "1.5 days for the whole database of 1000000 inputs.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run all courutines\n",
    "init_idx = 21811\n",
    "nb_tweets = 10\n",
    "\n",
    "start = time.time()\n",
    "results = asyncio.run(run_all(start=init_idx, end=init_idx + nb_tweets))\n",
    "elapsed = time.time() - start\n",
    "\n",
    "whole_db_nb_tweets = int(1e6)\n",
    "\n",
    "\n",
    "print(f\"\"\"\n",
    "Annotationg {nb_tweets} tweets took {elapsed} seconds,\n",
    "this is {elapsed / nb_tweets} seconds per tweet or\n",
    "{(1 / (24 * 3600)) * whole_db_nb_tweets * elapsed / nb_tweets:.1f} days for the whole database of {whole_db_nb_tweets} inputs.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb305dbf-ff70-4e96-a770-df858e91ca0e",
   "metadata": {},
   "source": [
    "> zephyr:7b\n",
    ">\n",
    "> Annotationg 5 tweets took 2.760875940322876 seconds,\n",
    "> \n",
    "> this is 0.5521751880645752 seconds per tweet or\n",
    ">\n",
    "> 6.3 days for the whole database of 1000000 inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1279840-b095-422e-9452-8d67dd0dff49",
   "metadata": {},
   "source": [
    "> llama3.1:8b\n",
    "> \n",
    "> Annotationg 10 tweets took 1.274366855621338 seconds,\n",
    ">\n",
    "> this is 0.12743668556213378 seconds per tweet or\n",
    ">\n",
    "> 1.5 days for the whole database of 1000000 inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8b4888-aa91-4938-8eaf-b416d9b55f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet, answer in zip(tweets, [r.choices[0].message.content for r in results]):\n",
    "    if answer == \"Trump\":\n",
    "        print(\"-----------------------------------------------\" + \"\\n\" + f\"tweet: {tweet}\" + \"\\n\" + f\"answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cfea5c-b2c6-46d0-a220-7f51db66d2b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13 (Ollama)",
   "language": "python",
   "name": "rag_llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
